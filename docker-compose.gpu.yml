version: "3.9"

services:
  # Ollama con GPU
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: ollama_gpu
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    ports:
      - "11435:11434"  # Mapea el 11435 (host) al 11434 (contenedor)
    
    # --- INICIO DEL ARREGLO ---
    volumes:
      # En lugar de montar una carpeta local (./models_gpu),
      # usa un volumen nombrado. Docker lo gestionará.
      - ollama_gpu_data:/root/.ollama
      # 2. EL ARREGLO: Un "portal" de solo lectura a tus Modelfiles de GPU
      - ./actions/models/pompi_pesado_gpu:/modelfiles_gpu
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia      # <-- Faltaba esto
              count: all         # <-- Y esto (o '1')
              capabilities: [gpu]

# --- AÑADE ESTO AL FINAL ---
# Define el volumen nombrado
volumes:
  ollama_gpu_data: