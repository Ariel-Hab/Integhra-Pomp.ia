version: "3.9"

services:
  # 1️⃣ Rasa Core (cerebro)
  rasa-server:
    # --- CAMBIO ---
    # Ya no usa "build:", ahora "jala" la imagen desde GHCR
    image: ghcr.io/ariel-hab/integhra-pomp.ia:rasa-1.0
    container_name: rasa_server
    ports:
      - "8000:8000"
    # --- IMPORTANTE ---
    # Mantenemos el volumen para que puedas editar tu
    # código localmente y se refleje DENTRO del contenedor
    volumes:
      - ./bot:/app/bot
    working_dir: /app/bot
    command: ["/app/.venv/bin/python", "main.py"]
    environment:
      - ACTION_SERVER_URL=http://actions-server:5055/webhook
    depends_on:
      - actions-server

  # 2️⃣ Rasa Actions (lógica)
  actions-server:
    # --- CAMBIO ---
    # Ya no usa "build:", ahora "jala" la imagen desde GHCR
    image: ghcr.io/ariel-hab/integhra-pomp.ia:actions-1.0
    container_name: actions_server
    # --- IMPORTANTE ---
    # Mantenemos el volumen para el desarrollo local
    volumes:
      - ./actions:/app/actions
    env_file:
      - .env.local
    depends_on:
      - ollama-cpu

  # 3️⃣ Ollama (modelo CPU)
  ollama-cpu:
    image: ollama/ollama:latest # (Este ya estaba bien)
    container_name: ollama_cpu
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    ports:
      - "11434:11434"
    volumes:
      - ollama_cpu_data:/root/.ollama
      - ./actions/models/pompi_liviano_cpu:/modelfiles_cpu

volumes:
  ollama_cpu_data: