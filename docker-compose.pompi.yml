version: "3.9"

services:
  # 1. El Cerebro (Tu main.py)
  rasa-server:
    build:
      context: .
      dockerfile: Dockerfile.bot
    container_name: rasa_server
    # command: "poetry run python main.py"
    ports:
      - "8000:8000" # Expones tu API de main.py al mundo
    volumes:
      - .:/app # Monta todo tu código para que main.py funcione
    environment:
      # Usa el nombre del servicio de Docker para la comunicación interna
      - ACTION_SERVER_URL=http://actions-server:5055/webhook
    depends_on:
      - actions-server

  # 2. La Lógica (Tus actions)
  actions-server:
    build:
      context: .
      dockerfile: Dockerfile.actions
    container_name: actions_server
    volumes:
      - ./actions:/app/actions
    environment: 
      # --- Variables Externas (Leídas desde .env) ---
      
      # Apunta a la IP PRIVADA de tu Servidor B (Ollama)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      
      # Apunta a tu API de productos (Integhra)
      - API_CLIENT_BASE_URL=${API_CLIENT_BASE_URL}
      
      # Apunta al rasa-server (para streaming, si lo usas)
      - RASA_API_URL=${RASA_API_URL}
    # No es necesario exponer el puerto 5055, 
    # ya que solo rasa-server habla con él.