version: "3.9"

services:
  # 1. El Músculo (LLM)
  ollama-server:
    image: ollama/ollama
    container_name: ollama_server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Si tuvieras GPU en el Droplet, añadirías aquí la sección "deploy"

  # 2. El Cerebro (Rasa NLU/Core)
  rasa-server:
    build:
      context: .
      dockerfile: Dockerfile.bot
    container_name: rasa_server
    environment:
      - ACTION_SERVER_URL=http://actions-server:5055/webhook
    ports:
      - "5005:5005" # Puerto estándar de Rasa para la API
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    command: "poetry run rasa run --enable-api --cors \"*\" -p 5005"
    depends_on:
      - actions-server

  # 3. La Lógica (Actions + Langchain)
  actions-server:
    build:
      context: .
      dockerfile: Dockerfile.actions
    container_name: actions_server
    environment:
      # Así tu model_manager.py sabe dónde encontrar a Ollama
      - OLLAMA_BASE_URL=http://ollama-server:11434/v1
    ports:
      - "5055:5055"
    volumes:
      - ./actions:/app/actions # Montamos solo la carpeta de actions
    command: "poetry run rasa run actions"
    depends_on:
      - ollama-server

volumes:
  ollama_data: # Volumen para guardar los modelos de Ollama